---
title:  "[PyTorch] 기본 & 구조"
date:   2022-01-26 02:00:00
categories:
- 네이버 부스트캠프 ai tech 3기
tags:
- 부스트캠프
- boostcamp
- 부캠_학습정리
- 글쓰기
---

### 확률론과 통계학

우선 지난 번에 정리하지 못한 확률론과 통계학 부분 강의에 대한 정리.



**확률론**

딥러닝은 확률론 기반의 머신러닝 이론.  
회귀분석에서 손실함수로 사용되는 L2노름은 예측오차의 분산을 최소화하는 방향으로 학습하도록 유도한다.

이산형확률분포와 연속형확률분포. 원래 확률분포와 상관없이 결합분포는 다를 수 있다 

주변확률분포(P(X))보다는 조건부확률분포가 통계적 관계를 모델링하기 좋다.

연속확률분포의 기대값은 주어진 함수에 확률밀도함수를 곱한 후 적분, 이산확률분포의 기대값은 주어진 함수에 확률질량함수를 곱한 후 급수를 사용한다.

로지스틱 회귀에서 사용된 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는 데에 사용된다.

확률분포를 모를 때는 몬테카를로 샘플링으로 기대값을 계산한다. 이산형이든 연속형이든 상관 없다.
독립적으로 샘플링을 해야만 한다. 독립추출이라는 전제 하에 대수의 법칙에 따라 목표하는 기대값을 얻을 수 있다.



**통계학**

통계학 맛보기

데이터가 특정 확률분포를 따른다고 가정하면 모수적 방법론.

표집분포는 통계량의 확률분포. 표본들의 분포가 아니다! 표본평균과 표본분포를 말한다. n이 커질수록 정규분포....중심극한정리.

최대가능도 추정법은 가장 가능성이 높은 모수를 추정하는 방법. 이 가능도 함수는 모수를 따르는 분포가 x를 관찰할 확률이 아닌 가능성을 뜻하는 것이다. 즉 대소비교가 가능한 값.

데이터 집합이 독립 추출일 때는 로그가능도를 최적화. 로그를 사용한다면 가능도의 곱셈을 로그가능도의 덧셈으로 계산 가능. 따라서 시간복잡도를 획기적으로 줄여준다. 단, 경사하강법을 사용하는 손실함수는 음의 로그가능도 사용.


---------------------------------------------

### PyTorch 기본과 구조

- numpy 기반이라 사용 방식이 상당히 유사하다.
- AutoGrad를 이용해서 자동 미분을 지원한다.
- 딥러닝을 지원하는 함수와 모델이 다양하다.

프레임워크를 공부하는 것이 딥러닝을 공부하는 것이다
파이토치와 텐서플로우가 메인스트림!

케라스는 wrapper. 케라스 자체가 딥러닝을 지원하는 건 아니라 내부에서 파이토치나 텐서플로우 같은 걸 지원함. 

사실상 텐서플로우랑 케라스랑 합쳐짐. 텐서플로우랑 파이토치의 가장 큰 차이점은 그래프 표현 방식이다. 

텐서플로우는 정적 그래프(define and run)를 그린다. 그래프를 먼저 코드로 정의한 후에 실행 시점에 데이터를 넣어준다. 

반면에 파이토치는 동적 그래프라서 먼저 연산의 과정을 그래프로 그린다. 실행하면서 그래프 생성. 파이토치 방식이 디버깅하기 더 쉽다고 한다.

<br/>
<br/>

### Tensor

앞서 말했듯이 numpy와 상당히 유사하다.
```python
import numpy as np
n_array = np.arange(10).reshape(2, 5)
print(n_array)
print("ndim :", n_array.ndim, "shape :", n_array.shape)
```
위와 아래 코드의 출력 결과는 같다.
```python
import torch
t_array = torch.FloatTensor(n_array)
print(t_array)
print("ndim :", t_array.ndim, "shape :", t_array.shape)
```

리스트나 ndarray로도 Tensor 생성이 가능하다.
```python
data = [[1, 3], [5, 7]]

l_tensor = torch.tensor(data)
l_tensor


nd_array = np.array(data)
a_tensor = torch.from_numpy(nd_array)
a_tensor

```
<br/>

#### 아무튼 numpy처럼 작동한다.

이렇게
```python
data = [[1, 2, 3],[4, 5, 6], [7, 8, 9]]
tensored = torch.tensor(data)

tensored[1:] # 이렇게 슬라이싱 가능
tensored[:2, 1:]

tensored.flatten() # numpy에서처럼 이렇게 쫙 펴는 것도 가능

torch.ones_like(tensored) # 마찬가지로 이렇게 1로 채워넣는 것도 똑같다

tensored.numpy()

tensored.shape

tensored.dtype
```
<br/>

#### Tensor handling

- view : tensor의 shape를 변환한다. reshape와 유사하다.
- squeeze : 차원의 개수가 1인 차원을 삭제한다.
- unsqueeze : 차원의 개수가 1인 차원을 추가한다.

먼저 view와 reshape의 차이를 간단히 말하자면 깊은 복사(reshape)와 얕은 복사(view)의 차이이다.
```python
# 만약에 충분히 큰 크기를 가진 data라는 tensor가 있다면
data.view([-1, 3])
data.reshape([-1, 3])
# 전자와 후자의 결과값은 같다.

# 하지만..! 아래의 코드를 실행시키면 차이가 보일 것이다.

a = torch.zeros(3, 2)
b = a.view(2, 3)
a.fill_(1)

a = torch.zeros(3, 2)
b = a.t().reshape(6)
a.fill_(1)

```
contiguity 차이가 있는 것인데, 
