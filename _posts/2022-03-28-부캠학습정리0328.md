---
title: "GPT"
date: 2022-03-29 12:00:00 +0900
categories:
- 네이버 부스트캠프 ai tech 3기
tags:
- 부스트캠프
- boostcamp
- 부캠_학습정리
- 글쓰기
---

# GPT(Generation Pre-trained Transformer)


BERT는 transformer의 인코더 사용, GPT는 디코더를 사용한다. 다음 단어 예측이나 묻고 답할 수 있는 챗봇 생성과 같은 기능을 기대할 수 있다.

- GPT는 자연어 문장을 분류하는 작업에서 좋은 성능을 내는 디코더.
- 적은 양의 데이터에서도 높은 분류 성능을 보여줌.
- 다양한 자연어 task에서 SOTA 달성.
- Pre-train 언어 모델의 새 지평을 열었고, BERT 언어 모델 발전의 밑거름이 됨.
- 하지만 여전히, 지도 학습을 필요로 하며, labeled data가 필수.
- 특정 task를 위해 fine-tuning 된 모델은 다른 task에서 사용 불가능

그리고 위와 같은 GPT의 특징에서 새로운 결론에 도달한다. 언어의 특성을 고려했을 때, 지도 학습의 목적 함수는 비지도 학습의 목적 함수와 같다. 따라서 fine-tuning이 필요없다. 다시 말하자면 인간이 어린 시절에 언어를 학습하는 원리와 비슷하다고 볼 수 있다. 어떠한 언어로 이루어진 글 따위를 계속 접하게 되면 결국 그 언어에 익숙해질 것이다. 즉, 방대한 데이터셋을 학습시키면 언어 모델도 자연어 task를 자연스럽게 학습할 것이다. one-shot zero-shot few-shot

<br/>

# GPT 기반 자연어 생성법

대표적인 디코딩 방법으로 Greedy search, Beam search, Top-K sampling, Top-p sampling이 있다.

### Greedy search
Greedy search는 단순히 가장 높은 확률을 가진 단어를 다음 단어로 선택한다.


### Beam search
Beam search는 각 time step에서 가장 확률이 높은 hypotheses의 num_beams를 유지하고, 전체 확률이 가장 높은 hypothesis를 선택해서 숨겨진 높은 확률 word sequence를 놓칠 위험을 줄일 수 있다. Greedy search보다 높은 확률의 결과 sequence를 찾는 것이 가능하다만, 물론 이것이 가장 뛰어난 결과를 보장하진 않는다. 추측되는 이유는 다음과 같다.

1. Beam search는 machine translation 또는 text summarization처럼 원하는 문장 생성 길이가 예측 가능한 task에서는 잘 작동할 수 있지만 dialog 또는 Story Generation Task처럼 출력길이가 크게 달라질 수 있는 개방형 생성에서는 원활하게 작동하지 않는다.

2. Beam search은 반복 생성 문제에 취약하다. 특히 Story Generation Task에서 n-gram또는 기타 패널티를 통해 문장을 제어하는 것이 어렵다고 한다. 왜냐하면 "반복이 없는 구문"과 "n=gram반복 주기" 사이에서 적당한 trade-off를 찾기 위해 많은 finetuning이 필요하기 때문이다.

3. 마지막으로 실제 인간의 언어에 가까울수록 기존 언어 모델이 채용하고 있는 높은 확률의 단어를 선택하는 방식을 쓰지 않는다. 높은 수준의 언어는 지루하거나 예측 가능한 문장이 아니라 인간이 놀라게 할 수 있는 문장생성을 한다고 한다.


### Top-K sampling
Top-K sampling은 가능성이 가장 높은 k개의 후보를 선별한다. 그리고 확률 질량을 해당 K개의 다음 단어에 재분배한다. 스토리 생성에서 큰 효과를 보이며, GPT2에서 쓰인다.


### Top-p (nucleus) sampling
가장 가능성이 높은 k개의 단어를 샘플링하는 대신에 누적 확률이 확률 p에 다다르는 최소한의 단어 집합으로부터 샘플링한다. 가장 높은 확률을 가지는 토큰부터 시작해서 확률 값의 합이 top-p값을 넘을 때까지 샘플링 풀에 토큰을 추가한다. 지금까지 소개한 방법 중에서 가장 좋다고 한다.

<br/>

# BERT 이후 Language Model

BERT
- MASK 토큰을 독립적으로 예측
- 토큰 간의 관계 학습 불가
- embedding length의 한계로 인해 segment 간 관계 학습 불가

GPT
- 단일 방향으로만 학습

BERT와 GPT는 위와 같은 문제점이 있다.

### XLNet 
- Relative Positional Encoding : sequece 길이 제한이 풀림.
- Permutation language modeling : 순열조합으로 문장 학습한다. 문장이 주어지면 그 문장의 단어를 섞어 학습..? 마스크 필요없어.


### RoBERTa

### BART
- 트랜스포머 인코더 디코더 통합
- BERT랑 GPT 섞었다고 보면 될 듯..마스킹한 토큰도 예측하고, 올바른 단어 순열(문장) 예측 등등

### T-5
- 트랜스포머 인코더 디코더 통합
- 얘는 마스킹하긴 하는데 어절 자체를 마스킹한다. 복원 작업도 어절 전체를 복원.

### Meena
- 대화 모델을 위한 언어 모델..이루다 같은데 이루다처럼 스스로 질문도 하는 듯
- 챗봇 평가 척도인 SSA 제시(Sensibleness and Specificity Average)


### Controllable LM - Plug and Play Language Model
기본적으로 다음에 나올 단어들을 확률 분포를 통해 선택.

근데 Plug and Play Language Model에선 내가 원하는 단어들의 확률이 최대가 되도록 이전 상태의 벡터 수정..수정 후 다음 단어 예측.
- 확률 분포 사용하기에, bag of words를 중첩 가능.
- 게다가 특정 카테고리(정치, 종교, 범죄)에 대한 감정을 컨트롤 해서 생성 가능
- 확률 분포 조절을 통해 그라데이션 분노 가능



### Multi-modal Language Model
다양한 학습 대상을 선정. 그 어떤 것도 학습 대상이 될 수 있다.
어떤 학습 대상에 반응하는지에 주목하기?

예를 들어 이미지와 자연어 동시에 학습
