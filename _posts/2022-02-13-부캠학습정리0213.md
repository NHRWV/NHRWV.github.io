---
title:  "[DL Basic] RNN"
date:   2022-02-13 00:03:00
categories:
- 네이버 부스트캠프 ai tech 3기
tags:
- 부스트캠프
- boostcamp
- 부캠_학습정리
- 글쓰기
---

Recurrent Neural Networks

### Sequential Model

길이가 언제 끝날지 모르는 연속되는 데이터라서 어렵다. 그래서 몇 개의 입력이 있든지 동작할 수 있는 모델을 만들어야 한다. 그래서  

- naive sequence model : 다음번 입력이 무엇이 될지 예측하는 모델
- autoregressive model : 과거의 입력을 어디까지 볼 것인가라는 질문...예를 들어 최근 다섯 개까지만 보겠다
- markov model : 바로 직전 상황만 보는 것
- latent autoregressive model...하나의 과거 정보에 의존하긴 하는데 그게 이전 과거를 요약한 hidden state


### RNN(Recurrent Neural Network)
MLP와 유사하다. 다만 자기에게 돌아오는 구조가 하나 더 있다. RNN을 시간순서로 풀어놓으면 입력이 많고 width가 커진 fully connected layer라고 볼 수 있다.
따라서 가까운 과거의 정보는 잘 반영하지만 long-term dependency(과거의 정보를 취합해야 하는데 먼 과거의 일을 고려하기 힘들다.) 문제를 해결하지 못하는 단점이 있다.


### LSTM(Long Short Term Meomory)
이 아이가 RNN의 단점 long-term dependency를 잡기 위한 존재다.

<img src="https://i.imgur.com/oedq83X.png" width="600" height="300"/>

- previous cell state : 현재 시점(0 ~ t)까지 입력받은 정보를 취합한 것. 밖으로 나가진 않는다.
- previous hidden state : 밖으로 나가서 다음 번에도 previous hidden state가 된다. 결국 인풋과 아웃풋이 세 개다. 다만 실제로 나가는 건 ouput밖에 없다.
- forget gate : 어떤 정보를 버릴지 정함.
- input gate : cell state에 어떤 정보를 올릴지 정함 previous hidden state와 현재 input을 잘 섞어서 업데이트
- update cell : 새로운 cell state 갱신
- output gate : 어떤 값을 밖으로 내보낼지 정한다...next hidden state


### GRU(Gated Recurrent Unit)
얘는 게이트가 두 개다(reset gate, update gate).
hidden state가 곧 output이며 cell state가 없다.  
LSTM보다 파라미터 수가 적기 때문에 일반적으로 성능이 좋은 경우가 많다.

<br/>

# Transformer

순서가 뒤바뀌거나, 중간 내용이 생략되었거나, 혹은 부분적으로 잘린 내용이 있다면 모델링이 어렵다.
이러한 sequential data를 처리하고 인코딩하는 걸 다루는 방법론이 transformer.  

이는 재귀적 구조가 아닌 attention이라는 새로운 구조를 가진다.

입력 시퀀스와 출력 시퀀스의 도메인(언어..?), 단어 숫자가 다를 수 있다. 몇 개의 단어가 입력 되든지 그 숫자만큼 재귀적으로 돌아가는 게 아니라 한 번에 처리하는 구조이다. 동일한 학습구조를 가지는 독립된 인코더와 디코더의 스택으로 구성된다.

encoder = Self-Attention + Feed Forward Neural Network
self-attention은 encoder와 decoder 양쪽에 다 있는 파트이며, transformer의 주춧돌.

### encoding

1. 만약 n개의 단어가 입력 되면, 각 단어마다 벡터로 표현된다. 이때 self-attention은 그 단어 벡터들을 각각 n개의 feature 벡터로 인코딩한다. 중요한 점은 하나의 벡터가 feature 벡터로 인코딩되는 이 과정에서 다른 n-1 개 벡터를 함께 고려한다. 즉, self-attention은 문장을 해석할 때 각각의 단어가 문맥적 상황에서 어떤 의미를 가지는지 파악할 수 있도록 한다.
<img src="https://i.imgur.com/9VwwBqX.png" width="800" height="300"/>

2. 셀프어텐션에서는 어떤 단어가 주어졌을 때 세 가지의 벡터를 생성한다. 생성된 이 세 개의 뉴럴네트워크는 각각 Queries, Keys, Values.
 
3. 그리고 score 벡터를 만들게 된다. 인코딩할 때 한 단어의 score 벡터는 그 단어의 qurie 벡터와 나머지 단어의 key 벡터들과의 내적으로 구한다. 예를 들어 q1 * k1, q1 * k2 ....즉, 특정 단어가 나머지 단어와 얼마나 연관이 있는지를 구하게 된다(유사도).

4. 그런 다음에 score 벡터를 key 벡터 차원 개수에 제곱근을 씌운 값만큼 나눠서 정규화한다.

5. 여기에 softmax 함수를 적용하면 이게 attention weights가 된다. 이 attention weights에 value 벡터를 각각 곱해서 나온 합을 구하면(가중치 합) 단어 하나의 인코딩 완료. 이때 인코딩 된 벡터의 차원은 value 벡터 차원과 같다.

<img src="https://i.imgur.com/wx1KEA7.png" width="600" height="400"/> 

<br/>

아래 그림이 위 과정의 요약이다.

<img src="https://i.imgur.com/nQtsR4T.png" width="600" height="300"/>



encoding 마지막으로
- positional encoding :
input에 bias 같은 특정한 값을 추가하는 것이다. transformer 특성상 attention은 뽑아내지만 단어의 순서에 대한 정보는 들어있지 않기 때문이다. 그래서 미리 정해져 있는 값들을 순서에 맞게 더해준다.


위의 과정이 모두 끝났다면 decoding 하면 된다!! autoregressive 방법으로 디코딩 된다.


### decoding

encoder에서 가져온 정보로 문장 따위를 생성한다.

1. 마지막 encoder의 Key, Value 벡터가 decoder로 전달
2. decoder에 들어가는 Query 벡터와 encoder의 Key, Value 벡터로 autoregressive하게 결과물이 하나씩 만들어지게 된다.
3. 이때 미래에 있는 정보를 활용하지 않기 위해 masking 과정을 거친다.


