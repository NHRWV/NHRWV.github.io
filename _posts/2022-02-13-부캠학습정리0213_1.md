---
title:  "[DL Basic] Generative Model"
date:   2022-02-13 19:42:00
categories:
- 네이버 부스트캠프 ai tech 3기
tags:
- 부스트캠프
- boostcamp
- 부캠_학습정리
- 글쓰기
---


### 당장 이해가 안 되어서 추후에 정리할 계획

generative model

generation 새로운 학습 데이터를 만들어내기
density estimation 어떤 이미지가 있을 때, 이게 어떤 클래스인지 구분하는 것
 따라서 explicit 모델이다. 입력이 주어졌을 때 확률값을 얻을 수 있는 모델이다.
unsupervised representation learning....feature learning이 가능하다. 공통적으로 눈, 코, 귀를 가지고 있다는 걸 알아차린다든가

discrete distribution
rgb 케이스에서 하나의 픽셀이 가질 수 있는 경우의 수는 256^3
필요한 파라미터 개수는 256^3 - 1
즉, 하나의 rgb 픽셀을 표현하기 위해서 필요한 파라미터 개수가 많다.

binary 형식의 픽셀을 가진 이미지가 있다면, n개의 픽셀이 있으면 2^n의 경우의 수에 2^n - 1의 파라미터가 필요


그런데 말이지, 만약 모든 픽셀이 독립적이라고 가정한다면? 가능한 경우의 수는 여전히 2^n이지만 필요한 파라미터 수는 n개. 각각의 픽셀에 대해 파라미터는 한 개가 필요한데, n개의 픽셀이 모두 독립적이라 그냥 더하면 된다.

conditional independence 
이거 파라미터 세는 법 영상 참고


auto-regressive model
어떤 정보가 이전 정보와 독립적이지 않을 걸 말한다. 따라서 마르코프도 여기에 속하긴 하다.

variational inference의 목적은 posterior distribution(관측값이 주어졌을 때 내가 관심 있는 것을 random variable의 확률 분포)을 찾는 것이다.
variational distribution은 포스터리어 디스트리뷰션을 현실적으로 구하기가 어려워서 근사하는 분포
그러니까 variational inference의 목적은 variational distribution 찾기


gan의 가장 큰 장점 : 학습의 결과로 나오는 generator를 학습하는 discriminator 성능이 점점 좋아진다.
