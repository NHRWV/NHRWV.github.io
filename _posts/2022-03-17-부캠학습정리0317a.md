---
title: "[NLP] Transformer"
date: 2022-03-18 04:00:00 +0900
categories:
- 네이버 부스트캠프 ai tech 3기
tags:
- 부스트캠프
- boostcamp
- 부캠_학습정리
- 글쓰기
---

## Transformer

seq2seq 모델은 인코더에서 입력 시퀀스를 context vector로 압축하고, 디코더는 이를 통해 출력 시퀀스를 만들어냈다.

그러나 seq2seq 모델은 결국 RNN을 기반했기에 크게 두 가지 문제가 있다.

1. 하나의 고정된 크기의 벡터(context vector)에 모든 정보를 압축하는 과정에서 정보 손실이 발생.
2. 기울기 소실(vanishing gradient) 문제.

위와 같은 문제를 해결하기 위해 등장한 게 attention. attention 구조에서는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 매 번 참고한다. 다만 해당 시점에서 예측해야할 단어와 가장 연관이 있는 입력 단어를 더 집중(attention)해서 본다. 따라서 인코더에 입력되는 시퀀스의 길이가 길어져도 괜찮다.


하지만 seq2seq에서 사용된 attention은 그저 서포터 역할을 수행하는 add on 모듈로써 사용되었다. 한편 Transformer는 attention만으로 인코더와 디코더를 만들게 된다. 
Attention is all you need!



### Bi-Directional RNNs

특정 단어의 왼쪽에 있는 정보뿐만 아니라 오른쪽에 있는 정보도 같이 포함할 수 있도록 forward RNN과 backward RNN 두 개의 모델을 만들고, 매 time step의 양쪽 모듈에서 가져온 hidden state vector를 concat한다면 기존 정보의 두 배에 해당하는 sequence 전체의 정보를 반영한 encoding vector를 얻을 수 있다.


### Transformer

추후 추가 예정...아직 이해를 못했다.




### Transformer : Multi-Head Attention

multi-head attention은 self-attention을 유연하게 확장한 것이다.

- Multi-head attention maps 𝑄, 𝐾, 𝑉 into the ℎ number of lower-dimensional spaces via 𝑊 matrices.
- Then apply attention, then concatenate outputs and pipe through linear layer.

쉽게 말하자면 단어 벡터의 차원을 ℎ개로 나눠서 (단어 벡터 차원 / ℎ)개의 차원을 가지는 𝑄, 𝐾, 𝑉에 대해서 ℎ번의 병렬 어텐션을 수행한다는 것이다.
각각의 attention 값 행렬을 attention head라고 부르고, 이때 사용되는 가중치 행렬 𝑊<sub>𝑄</sub>, 𝑊<sub>𝐾</sub>, 𝑊<sub>𝑉</sub> 도 head 개수만큼 존재한다. 즉, i번째 attention 수행에는 i번째 가중치 행렬들이 사용된다. attention의 개수(head 개수)만큼 인코딩 벡터가 나오는데, 이러한 인코딩 벡터를 concat해서 해당 쿼리 벡터에 대한 encoding 벡터를 얻는다.

attention 과정을 병렬적으로 수행함으로써 여러 관점에서 정보를 수집할 수 있다.


### 효율성
- n is the sequence length
- d is the dimension of representation
- k is the kernel size of convolutions
- r is the size of the neighborhood in restricted self-attention

<img src="https://i.imgur.com/HFMs0uD.png" width="600" height="150"/>

self-attention 기반의 transformer 모델은 RNN보다 시간복잡도 측면에서 낫지만 메모리를 더 요구한다.
maximum path length의 측면을 살펴보자면 RNN은 학습을 위해 최대 n번의 layer를 통과해야 한다(O(n)). 반면에 self-attention은 인접한 아이를 보면 되기에 1이다.



### Transformer : Layer Normalization

transformer에 추가적으로 쓰이는 작업으로 잔차 연결(Residual connection)과 층 정규화(Layer Normalization)가 있다.

먼저 residual connection은 기울기 소실 문제를 해결하여 layer를 계속 쌓아감에 따라 더 높은 성능을 내는 동시에 학습은 안정화한다. trnasformer에서 layer의 입력과 출력은 똑같은 차원을 가지기 때문에 덧셈 연산이 가능하다. 이를 이용해서 residual connection은 sub layer의 입력과 출력을 더하는 것이다. 주로 CV쪽에서 활용된다고 한다.

그리고 residual connection을 거쳐서 Layer Normalization을 수행한다.

Layer normalization changes input to have zero mean and unit variance, per layer and per training point.

1. 열 벡터를 기준으로 평균을 0, 분산을 1로 만든다.
2. layer에서 발견된 각 노드별로 원하는 평균과 분산을 주입하는 동일한 affine transformation 과정을 수행한다.

<img src="https://i.imgur.com/pY9ODa9.png" width="700" height="280"/>

<br/>

참고로,  

<img src="https://i.imgur.com/6LQ3zoc.png" width="700" height="200"/>

다만, 배치와 레이어 정규화의 차이를 모르겠다. 각각 어떤 경우에 사용하는지도 모르겠다.


### Transformer : Positional Encoding

attention 구조에서는 key, value 벡터들이 각 key 별로 주어진 query와의 attention 유사도를 구하고, 해당 value 벡터에 가중치를 부여해서 가중치 합을 구함으로써 주어진 입력 벡터에 대한 encoding output vector를 얻게 되는데, 가중평균을 내는 과정에서 value vector들의 순서가 상관 없다는 특징이 있다. 다시 말하자면 self-attention은 어떤 벡터의 집합이 있을 때 벡터끼리 서로 전부 비교한 결과를 다시 벡터의 집합으로 돌려준다. 그래서 벡터가 어떤 순서로 있는지는 self-attention의 연산에서 신경쓰지 않는다는 것이다. 따라서 RNN과 달리 Transformer(self-attention)모델은 순서를 반영하지 못한다.

입출력 순서를 따져야 하는 경우에 뭔가 대책이 필요하다. -> positional encoding.

positional encoding : 순서를 구분할 수 있는 unique한 상수 벡터를 각 순서에 등장하는 단어의 입력 벡터에 더해준다. 결국 위치가 달라지면 출력 encoding 벡터도 달라진다.



### Transformer : Warm-up Learning Rate Scheduler

learning rate를 고정하지 않고, 학습 속도와 성능을 향상시킬 목적으로 learning rate를 학습 중에 적절히 변경하는 기법을 learning rate scheduler라고 한다.

작은 LR값으로 시작해서 점차 LR값을 크게 했다가 어느 시점부터는 다시 LR값을 줄여준다.



### Transformer : Decoder
추가 예정..




### Masked Multi-Head Attention

추후에 정리 예정

attention 구조상 모든 hidden vector를 동시에 사용하고, output 또한 모두 한 번에 얻을 수 있다. 
따라서 이전 시점까지의 정보를 바탕으로 다음 시점의 output을 추론하거나 생성해야 하는 language model에 사용하는 경우에는 모델이 과거만 참고하도록 미래 시점의 hidden state에 해당하는 것들을 masking 해줘야 한다.



