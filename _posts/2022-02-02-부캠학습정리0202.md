---
title:  "[PyTorch] 구조 & 활용"
date:   2022-02-02 17:00:00
categories:
- 네이버 부스트캠프 ai tech 3기
tags:
- 부스트캠프
- boostcamp
- 부캠_학습정리
- 글쓰기
---

## PyTorch Datasets & Dataloaders  

### Dataset 클래스

- 데이터 입력 형태를 정의하는 클래스
- 데이터를 입력하는 방식의 표준화
- Image, Text, Audio 등에 따른 다른 입력 정의


보통 밑의 코드와 같이 생겼다.  
```__init__```, ```__len__```, ```__getitem__```으로 구성.  
```python
import torch
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    # 초기데이터 생성방법 지정
    def __init__(self, text, labels):
        self.labels = labels
        self.data = text
	
    # 데이터의 전체길이 반환
    def __len__(self):
        return len(self.labels)

    # index값을 주었을 때 반환되는 데이터의 형태(X,y)
    def __getitem__(self, idx):
        label = self.labels[idx]
        text = self.data[idx]
        sample = {"Text": text, "Class": label}
        return sample
```

### Dataset 클래스 유의점

- 데이터 형태에 따라 각 함수를 다르게 정의함
- 모든 것을 데이터 생성 시점에 처리할 필요는 없음... 예를 들어, image의 Tensor 변화는 학습에 필요한 시점에 변환
- 데이터 셋에 대한 표준화된 처리 방법 제공하는데, 이는 후속 연구자 또는 동료에게는 빛과 같은 존재
- 최근에는 HuggingFace등 표준화된 라이브러리 사용


### DataLoader 클래스  

- Data의 Batch를 생성해주는 클래스
- 학습직전(GPU feed전) 데이터의 변환을 책임
- Tensor로 변환 + Batch 처리가 메인 업무
- 병렬적인 데이터 전처리 코드의 고민 필요

```python
text = ['Happy', 'Amazing', 'Sad', 'Unhapy', 'Glum']
labels = ['Positive', 'Positive', 'Negative', 'Negative', 'Negative']
MyDataset = CustomDataset(text, labels)

# 방법 1
MyDataLoader = DataLoader(MyDataset, batch_size=2, shuffle=True)
next(iter(MyDataLoader))

# 방법 2
MyDataLoader = DataLoader(MyDataset, batch_size=2, shuffle=True)
for dataset in MyDataLoader:
print(dataset)
```

### 그런데...DataLoader에 쓰이는 파라미터가 좀 많다

#### batch_size
- int, optional, default = 1

배치(batch)의 크기이다. 만약에 batch_size가 10일 때 데이터셋에 데이터가 100개라면, iteration 열 번만에 모든 데이터를 볼 수 있다.  
Tensor를 반환하기에 Tensor로 반환되지 않는 데이터라면 에러가 발생한다.

#### shuffle  
- bool, optional, default = False

데이터를 섞을지 말지 결정한다.   

#### sampler  
- Sampler, optional  

index 컨트롤을 하려면 써야 한다. 종류가 몇 가지 있는데 그 중 하나를 소개하자면,

WeightedRandomSampler: 각 샘플에 가중치를 부여해 샘플링한다.     

#### batch_sampler  
- Sampler, optional

이 아이는 잘 모르겠다. ```sampler```와 크게 차이가 없는 거 같다.    

#### num_workers  
- int, optional, default = 0

데이터를 불러올 때의 subprocess 개수를 설정한다. 
다만 값이 너무 높으면 CPU에서 GPU로의 교류에서 병목 현상이 발생할 수 있다.  

#### collate_fn  
- callable, optional

배치 단위의 데이터 변환할 때 사용한다고 한다. NLP처럼 길이가 가변적인 데이터를 사용할 때, zero-padding이나 Variable Size 데이터 등 데이터 사이즈를 맞추기 위해 많이 사용한다.  
데이터와 레이블을 따로 묶어서 출력할 때 사용한다.    

#### pin_memory  
- bool, optional

True로 설정하면 Tensor를 CUDA 고정 메모리에 올린다.    

#### drop_last  
- bool, optional

배치 사이즈에 따라 데이터를 불러온다면 마지막 배치의 크기가 달라질 수도 있다. 나머지가 생기는 현상인데, 이렇게 배치 사이즈에 맞게 나눠 떨어지지 않는 경우에 마지막 배치를 사용하지 않는다.     

#### time_out  
- numeric, optional, default = 0

데이터 불러오는 데에 제한 시간을 설정한다.    

#### worker_init_fn  
- callable, optional, default = ’None’

어떤 worker를 불러올 것인지 결정.


<br/>

## 모델 불러오기

### model.save()

- model.save 학습의 결과를 저장하는 함수
- 모델 형태와 파라미터 저장
- 덕분에 학습 중간 과정을 저장해서 가장 좋은 결과 모델을 선택할 수 있다.     

참고로..from torchsummary import summary ...모델 정보 보는 방법 중 하나       


### checkpoints  
학습의 중간 과정을 저장하여 최선의 결과를 선택할 수 있게끔 한다.  
early stopping 기법 사용 시에 이전 학습의 결과물 저장.  
epoch, loss, metric 값을 일반적으로 함께 저장.  
특히 colab 환경에서 지속적인 학습을 위해 필요하다.  

![chkpoint](https://imgur.com/IYAXQAR)



참고로 파일 읽어올 때 'UserWarning' 뜨는 경우 있는데, 이럴 땐
import warnings
warnings.filterwarnings("ignore")


### transfer learning

- 모델을 가져와서 쓰고 싶을 때 사용
- 다른 데이터셋으로 만든 모델을 현재 데이터에 적용하고 싶을 때 사용한다.
- 딥러닝에서는 일반적인 학습 방법.
- backbone architecture가 잘 학습된 모델을 부분 수정한 후 쓸 수 있다.
- 대용량 데이터셋으로 pretrained 된 좋은 모델을 가져온다. e.g) TorchVision, HuggingFace


### freezing 혹은 frozen?  

pretrained model 사용할 때 모델의 일부 과정을 고정시킨 후 더 이상 손 대지 않는 것.





